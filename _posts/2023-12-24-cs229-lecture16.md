<head>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            tex2jax: {
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
            inlineMath: [['$','$']]
            }
        });
    </script>
</head>

# CS229 Lecture 16
-----
> **强化学习
> 马尔可夫决策过程(MDPs)
> value function
> value iteration
> policy iteration**

-----
**强化学习**

与监督学习相比，强化学习没有所谓的标签，有的仅仅是一个反馈函数，根据当前的状态反馈一个是否正向的激励。



-----
**Markov Decision Process**

$MDP(S,A,P_{sa},\gamma,R)$ 五元组

$S$对应状态集合

$A$对应动作集合

$P_{sa}$是一个状态转换分布，给出如果在状态$s$选择动作$a$ 状态转换的分布，$P_{sa}$有以下性质:
> $\sum P_{sa}(s')=1,P_{sa}(s')\geq0$

$\gamma \in [0,1)$,是一个折扣因子

$R$ 是奖励函数，其实现$S\rightarrow \mathbb{R}$

**例子**:下面是一个地图

&nbsp;| &nbsp; | &nbsp; | +1
-------- |-------- |-------- |-------- 
&nbsp;| entry| &nbsp; | -1
&nbsp;| &nbsp; | &nbsp; | &nbsp;

上面$(2,2)$坐标未知有障碍物，机器人如何行走到$(4,3)$处或者$(4,2)$处。根据地图可知机器人可以在11个位置,机器人可以执行的动作为$\{N,S,E,W\}$。机器人在$(3,1)$位置处向北走的概率为$0.8$，向西走的概率为$0.1$，向东走的概率为$0.1$,因此有：

$p_{3,1}(3,2)=0.8$

$p_{3,1}(2,1)=0.1$

$p_{3,1}(4,1)=0.1$

$p_{3,1}(3,3)=0$

奖励函数:

$R((4,3))=+1$

$R((4,3))=+1$

$R((other \, state))=+1$

----
**value function**

马尔可夫决策过程(MDP)动态过程如下:

$s_0 \stackrel{a_0}\longrightarrow s_1 \stackrel{a_1}\longrightarrow s_2 \stackrel{a_2}\longrightarrow s_3 \stackrel{a_3}\longrightarrow \cdots \cdots$

首先以初始状态$s_0$开始，选择一个动作$a_0\in A$，作为动作执行的结果，在状态转换分布函数作用下状态变为$s_1\sim P_{s_0a_0}$,接着执行动作$a_1$，以此类推。

整个过程的总收益为：

$R(s_0)+ \gamma R(s_1)+\gamma^{2} R(s_2)+\cdots$

强化学习的目标就是随着时序选择合适的动作使得期望收益值最大

$E[R(s_0)+ \gamma R(s_1)+\gamma^{2} R(s_2)+\cdots]$

> Note that the reward at timestep $t$ is discounted by a factor of $\gamma$ . Thus, to make this expectation large, we would like to accrue positive rewards as soon as possible (and postpone negative rewards as long as possible). In economic applications where R(·) is the amount of money made, $\gamma$ also has a natural interpretation in terms of the interest rate (where a dollar today is worth more than a dollar tomorrow).

----
**policy function**

定义policy函数$\pi$为行为状态的函数映射，建议在该状态$s$下应该采取什么行动$a$,有$a=\pi(s)$，那么根据$policy$函数值函数的定义为:

$V^{\pi}(s)=E[R(s_0)+ \gamma R(s_1)+\gamma^{2} R(s_2)+\cdots| s_0=s,\pi]$

在上面的例子中，如果起始点在$(3,1)$，那么其最优策略见下表：	$policy$是一种基于当前状态使得期望收益最大的方法


$\rightarrow$ | $\rightarrow$ | $\rightarrow$ | +1
-------- |-------- |-------- |-------- 
$\uparrow$| entry| $\uparrow$| -1
$\uparrow$| $\leftarrow$ | $\leftarrow$ | $\leftarrow$

**如何计算最优的policy?**

定义符号$V^{\pi}$，$V^{\star}$,$\pi^{\star}$,	其总$\pi^{\star}$是最佳policy。对于$\pi$定义值函数$V^{\pi}:S\longrightarrow  \mathbb{R}$，因此值函数$V^{\pi}$在状态$s$执行$\pi$的期望总收为：

$V^{\pi}(s)=E[R(s_0)+ \gamma R(s_1)+\gamma^{2} R(s_2)+\cdots| s_0=s,\pi]$

例子:有策略如下：
$\rightarrow$ | $\rightarrow$ | $\rightarrow$ | +1
-------- |-------- |-------- |-------- 
$\downarrow$| entry| $\rightarrow$| -1
$\rightarrow$| $\rightarrow$ | $\uparrow$ | $\uparrow$

对应的价值函数为:

0.52 | 0.73 | 0.77 | +1
-------- |-------- |-------- |-------- 
-0.9| entry| -0.82| -1
-0.88|-0.87|-0.85|-1.0

$V^{\pi}(s)=E[R(s_0)+ \gamma (R(s_1)+\gamma R(s_2)+\cdots)| s_0=s,\pi]$

上式第一部分被称为及时奖励，第二部分成为未来奖励，第二部分实际上就是$V^{\pi}(s_1)$
$V^{\pi}(s)=R(s)+\gamma \sum_{s'}P_{s\pi(s')}(a)V^{\pi}(s')$

上式被称为$Bellman\,\,Equation$

给予一个固定的策略$\pi$如何求解$V^{\pi}(s)$=?

对于上面例子:

$V^{\pi}((3,1))=R((3,1))+\gamma[0.8V^{\pi}((3,2))+0.1V^{\pi}((4,1))+0.1V^{\pi}((2,1))]$

这些$V^{\pi}(s)$均是未知数，地图有11个状态因此可以写出11个线性方程。这样11个未知数11个约束，那么就有能解出$V^{\pi}$

---
**最优值函数**

$V^{{\star}}=\underset {\pi}{max}\,\, V^{\pi}(s)$

对应的贝尔曼方程版本为：

$V^{\star}(s)=R(s)+\underset {a}{max}\,\,\gamma \sum_{s'}P_{s\pi(s)}(s')V^{\star}(s')$

那么

$$\pi^{\star}={\underset {a}{\operatorname {arg\,max} }}\,\gamma \sum_{s'}P_{sa}(s')V^{\star}(s')$$

---
值迭代

初始 $V(s)=0\,\,\,\,\forall s$
迭代直到收敛{
&nbsp;&nbsp;&nbsp;&nbsp;$V(s):=R(s)+\underset {a}{max}\,\,\gamma \sum_{s'}P_{s\pi(s)}(s')V^{\star}(s')$
}

最终$V(s)\rightarrow V^{\star}(s)$,上述过程实现可以同步一次性更新所有也可以异步按顺序每次更细一个状态的值函数。

通过值迭代可以得到上面例子的$V^{\star}$:

0.86|0.9|0.96| +1
-------- |-------- |-------- |-------- 
0.82| entry| 0.69| -1
0.78|0.75|0.71|0.49

对应的$\pi^{\star}$是：
$\rightarrow$ | $\rightarrow$ | $\rightarrow$ | +1
-------- |-------- |-------- |-------- 
$\uparrow$| entry| $\uparrow$| -1
$\uparrow$| $\leftarrow$ | $\leftarrow$ | $\leftarrow$

在位置$(3,1)$时候之所以向西走更好：

$S:\sum_{s'}P_{s\pi(s)}(s')V^{\star}(s')=0.8*0.75+0.1*0.69+0.1*0.71=0.74$
$N:\sum_{s'}P_{s\pi(s)}(s')V^{\star}(s')=0.676$


---
**policy 迭代**

随机初始化$\pi$
迭代直到收敛{
&nbsp;&nbsp;&nbsp;&nbsp;(a) 让 $V:=V^{\pi}$
&nbsp;&nbsp;&nbsp;&nbsp;(b) 让 $\pi(s):={\underset {a\in A}{\operatorname {arg\,max} }}\,\gamma \sum_{s'}P_{sa}(s')V(s')$
}

-----

在不知道$P_{sa}$的时候怎么估计$P_{sa}$?

$P_{sa}(s')=\frac{\#times\, took\, we \,action\, a\, in\, state\, s \,and\, got \,to \,s′}{\#times\, took\, we \,action\, a\, in\, state\, s}$

当上式为$\frac{0}{0}$时，可以估计状态转移分布为$\frac{1}{S}$

----

**其迭代步骤为：**

随机初始化$\pi$

迭代直到收{
&nbsp;&nbsp;&nbsp;&nbsp;(a)多次执行策略$\pi$
&nbsp;&nbsp;&nbsp;&nbsp;(b)根据累积经验估计$P_{sa}$
&nbsp;&nbsp;&nbsp;&nbsp;(c) 基于前一步估计的$P_{sa}应用值迭代得到新的值函数$$V$
&nbsp;&nbsp;&nbsp;&nbsp;(d)基于贪心策略更新$\pi$
}





